{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing we'll go through today is analysis of the Demo Experiment data. We have data from 14 participants who completed the German version of the experiment.\n",
    "\n",
    "**Before we can start:**\n",
    "\n",
    "**a**) [Download the data](demo_data.zip) as a `.zip` file.\n",
    "\n",
    "**b**) Extract the data from the `.zip` file to a folder that makes sense for you. The data should be stored in that location in a folder called `data`.\n",
    "\n",
    "**c**) Start a new R script in RStudio.\n",
    "\n",
    "Once everyone has completed these steps, we'll begin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Load the Relevant Libraries\n",
    "\n",
    "These are the libraries we'll use for the analysis. Each library has a comment explaining what we will use it for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=3.5, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(readr)    # for reading the data into R\n",
    "library(purrr)    # for easily importing multiple files\n",
    "library(dplyr)    # for wrangling data (e.g., adding/renaming columns)\n",
    "library(tidyr)    # for switching between long and wide formats of data\n",
    "library(ordinal)  # for fitting CLMMs\n",
    "library(lme4)     # for fitting LMMs\n",
    "\n",
    "library(ggplot2)  # for visualising data\n",
    "theme_set(theme_bw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Import the Data\n",
    "\n",
    "Let's import the data extracted from the `.zip` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first, get a list of paths to all .csv data files\n",
    "data_paths <- list.files(\"data\", pattern=\".*\\\\.csv$\", full.names=TRUE)\n",
    "\n",
    "# now, iterate over these with `read_csv()` to import them\n",
    "# (note: col_types just says that we want these two columns to be stored as text, not numbers)\n",
    "raw_data <- map_df(data_paths, read_csv, col_types=c(participant=\"c\", frameRate=\"c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first few rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# print first 5 rows\n",
    "head(raw_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Format the Data\n",
    "\n",
    "Here we make two quick changes:\n",
    "\n",
    "1) We rename the column containing response variable to a short name: `resp`\n",
    "\n",
    "2) We create a new column, which will be the same variable but with the order of the responses hardcoded as a factor\n",
    "\n",
    "We store the resulting data frame in a new variable with a handy short name, `d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "d <- raw_data |>\n",
    "  # 1) rename to a shorter name\n",
    "  rename(resp = resp_scale.response) |>\n",
    "  # 2) set the factor levels explicitly\n",
    "  mutate( resp_fct = factor(resp, levels=1:5, ordered=TRUE) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding the variable as a factor in this last step is useful for the CLMM approach. It ensures that R knows the order of our Likert responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Clean the Data\n",
    "\n",
    "To remove outliers, we may want to filter by response times, or exclude participants who only provided one response for the whole experiment.\n",
    "\n",
    "Here are the rules we'll use:\n",
    "\n",
    "* *Rule a)* We will exclude participants who only pressed one button for the whole experiment\n",
    "\n",
    "* *Rule b)* We will exclude trials where participants were too fast to be paying attention (faster than 500 ms).\n",
    "\n",
    "* *Rule c)* We will exclude trials where participants were too slow to be paying attention (slower than 15 seconds).\n",
    "\n",
    "Remember that for your actual experiment, you will need to preregister these criteria.\n",
    "\n",
    "<br>\n",
    "We'll start by excluding participants who gave too few unique responses. To do this we first count the number of unique responses given by each participant. Then we can take from this a list of participants who only pressed one button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of unique responses from each participant\n",
    "participant_variety <- d |>\n",
    "  # select the relevant columns\n",
    "  select(participant, resp_fct) |>\n",
    "  # get unique responses\n",
    "  distinct() |>\n",
    "  # count unique responses per participant\n",
    "  count(participant) |>\n",
    "  # sort ascendingly\n",
    "  arrange(n)\n",
    "\n",
    "# get IDs of participants who only ever pressed one button\n",
    "bad_participants <- participant_variety |>\n",
    "  filter(n==1) |>\n",
    "  pull(participant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can filter our data to not include these bad participants. This code says that we should only keep participants who are not (`!`) in (`%in%`) the vector containing bad participant IDs (`bad_participants`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# a) exclude participants who only ever pressed one button\n",
    "d_clean_a <- filter(d, ! participant %in% bad_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding participants by Response Times (RTs) is much easier. We can say we want to keep trials where participants were under the maximum RT, and above the minimum RT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# b) exclude trials that were too slow\n",
    "d_clean_b <- filter(d_clean_a, resp_scale.rt < 15)\n",
    "\n",
    "# c) exclude trials that were too fast\n",
    "d_clean_c <- filter(d_clean_b, resp_scale.rt > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to calculate the number of participants/trials lost at each step. We can do this with the `nrow()` function to count the number of rows (i.e., trials) in the dataframe. To count the number of bad participants, we can use `length()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# there were 0 participants excluded on the basis of only ever pressing one button\n",
    "length(bad_participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# there were 14 trials excluded on the basis of slow responses\n",
    "nrow(d_clean_a) - nrow(d_clean_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# there were 0 trials excluded on the basis of fast responses\n",
    "nrow(d_clean_b) - nrow(d_clean_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data was quite good quality overall! ðŸ’ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Fit the Models!\n",
    "\n",
    "Now that the data are formatted and cleaned we can fit three models to estimate our norms:\n",
    "\n",
    "1. The traditional model (LM; `lm()`), where each item's ratings are averaged.\n",
    "\n",
    "2. A linear mixed-effects model (LMM; `lmer()`), where we norm via random effects, but still treat the ratings as continuous.\n",
    "\n",
    "3. A cumulative-link mixed-effects model (CLMM; `clmm()`), where we norm via random effects, on a latent normal distribution.\n",
    "\n",
    "### LM\n",
    "\n",
    "First, the traditional approach is to take a raw average of all ratings for each item (a simple linear model). A simple way of estimating this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# estimate a raw average for each item\n",
    "est_lm <- d_clean_c |>\n",
    "  group_by(word) |>\n",
    "  summarise(M = mean(resp)) |>\n",
    "  # store the model type as a variable (we'll use this later)\n",
    "  mutate(model = \"LM\")\n",
    "\n",
    "# print first 5 rows\n",
    "head(est_lm, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might not feel like it, but this is already a model (albeit a very simple one). A way of fitting this with the `lm()` function would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "m_lm <- lm(resp ~ 0 + word, data = d_clean_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to convince yourself that this is the same thing, then compare the output of `m_lm` with the estimates in `est_lm`. You should see exactly the same numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "m_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMM\n",
    "\n",
    "To norm via random effects in an LMM, we can use the `lmer()` function. We include random (varying) intercepts for participants (`(1 | participant)`) and items (`(1 | item)`). We will only extract estimates for items, but we also include random intercepts for participants because it usually improves the accuracy of the estimates for items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit LMM\n",
    "m_lmm <- lmer(\n",
    "  resp ~ 1 + (1 | word) + (1 | participant),\n",
    "  data = d_clean_c\n",
    ")\n",
    "\n",
    "# print result\n",
    "m_lmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the average rating overall (the `Intercept`) was 3.42. The standard deviations of the random effects tell us that words (`1.1517`) were more variable than participants (`0.2165`).\n",
    "\n",
    "We can now extract the estimated norms for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# extract estimates (item random effects)\n",
    "est_lmm <- ranef(m_lmm)$word |>\n",
    "  # put into a tidy dataframe\n",
    "  as_tibble(rownames = \"word\") |>\n",
    "  # M, standing for mean\n",
    "  rename(M = `(Intercept)`) |>\n",
    "  # store the model type as a variable (we'll use this later)\n",
    "  mutate(model = \"LMM\")\n",
    "\n",
    "# print first 5 rows\n",
    "head(est_lmm, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLMM\n",
    "\n",
    "To norm via random effects in a CLMM, we can use the `clmm()` function. We include the same random effects structure as above. The key difference is that now we are using a model more appropriate for ordinal ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit CLMM (probit-link for comparability)\n",
    "m_clmm <- clmm(\n",
    "  resp_fct ~ 1 + (1 | word) + (1 | participant),\n",
    "  data = d_clean_c,\n",
    "  link = \"probit\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "m_clmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us similar information to that seen above, but also gives us the locations of the thresholds on the latent distribution. For example, the threshold between the Likert responses of *3* and *4* (or, `3|4`) is at `-0.1341`.\n",
    "\n",
    "As with the LMM, we can extract norms for words from the CLMM like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# extract estimates (item random effects)\n",
    "est_clmm <- ranef(m_clmm)$word |>\n",
    "  # put into a tidy dataframe\n",
    "  as_tibble(rownames = \"word\") |>\n",
    "  # M, standing for mean\n",
    "  rename(M = `(Intercept)`) |>\n",
    "  # store the model type as a variable\n",
    "  mutate(model = \"CLMM\")\n",
    "\n",
    "# print first 5 rows\n",
    "head(est_clmm, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Compare the AICs\n",
    "\n",
    "One nice comparison we can do is on the Akaike Information Criteria (AIC). This is something we touched on in the session on [Empirical Research](https://jackedtaylor.github.io/expra-wise23/introduction/empirical_research.html). The AIC, and estimators like it, can help us decide which model is most appropriate for the trade-off between model complexity and model likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "AIC(m_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "AIC(m_lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "AIC(m_clmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the CLMM has the lowest AIC of the three models, and so is likely to be the most appropriate for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Join the Estimates Together\n",
    "\n",
    "Now that we have the estimates, we can join these rows together into one big dataframe, `norms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Stick the dataframes together, end-to-end\n",
    "norms <- bind_rows(est_lm, est_lmm, est_clmm)\n",
    "\n",
    "# print first 5 rows\n",
    "head(norms, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want the data in wide format (different model estimates are in different columns). This is possible with the `pivot_wider()` function. This lets us compare the estimates for each word from the three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# pivot the data into wide format\n",
    "norms_wide <- pivot_wider(norms, names_from=model, values_from=M)\n",
    "\n",
    "# print first 5 rows\n",
    "head(norms_wide, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Examine Correlations\n",
    "\n",
    "We may be interested in how similar our estimates are from our three approaches. The variables are all very highly correlated because they are all trying to capture very similar information. However, the CLMM approach is least like the other two (lower Pearson's r values in the matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "norms_wide |>\n",
    "  # remove non-numeric column\n",
    "  select(-word) |>\n",
    "  # get correlation matrix\n",
    "  cor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise these correlations with `ggplot2`\n",
    "\n",
    "First, the correlations between the LM and LMM approach are very linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "norms_wide |>\n",
    "  ggplot(aes(LMM, LM)) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method=\"lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the correlation between the CLMM and LMM estimates is very nonlinear. This reflects that the LM estimates are distorted by the LM method incorrectly treating the ordinal scale as though it is an interval scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "norms_wide |>\n",
    "  ggplot(aes(CLMM, LM)) +\n",
    "  geom_point() +\n",
    "  geom_smooth(method=\"lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Recreate the Plot in Pollock (2018)\n",
    "\n",
    "Let's see whether we can recreate the plot that Pollock ([2018](https://doi.org/10.3758/s13428-017-0938-y)) made of the data from Brysbaert et al. ([2014](https://doi.org/10.3758/s13428-013-0403-5))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first, calculate the means and SDs of raw ratings\n",
    "raw_summ <- d_clean_c |>\n",
    "  group_by(word) |>\n",
    "  summarise(M=mean(resp), SD=sd(resp))\n",
    "\n",
    "# then, show the Ms and SDs in a scatter plot\n",
    "raw_summ |>\n",
    "  ggplot(aes(M, SD)) +\n",
    "  geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even our small sample size shows a very similar pattern to that described by Pollock."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
